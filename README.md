# Optimizing GPT-2 for Summarization and Efficient Deployment

Implemented and evaluated advanced fine-tuning techniques, including Prompt Tuning, LoRA, and Traditional Fine-Tuning, to enhance GPT-2's performance on summarization tasks with metrics like evaluation loss and ROUGE scores.

Developed and applied quantization strategies such as Bitsandbytes and NF4, achieving significant reductions in memory usage and inference latency for deploying LLMs on constrained devices.
